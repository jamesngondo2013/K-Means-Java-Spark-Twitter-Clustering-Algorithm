package nuig.data.analytics.SparkTwitterClustering;

import org.apache.spark.ml.clustering.KMeansModel;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.clustering.KMeans;
import org.apache.spark.ml.linalg.Vector;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

public class App 
{
    public static void main( String[] args )
    {
    	SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("K-means Example");
        JavaSparkContext sc = new JavaSparkContext(conf);

        // creating a spark session and spark object
        SparkSession spark = SparkSession.builder()
        		  .master("local")
        		  .appName("my-spark-app")
        		  .config("spark.some.config.option", "config-value")
        		  .getOrCreate();
        
     // Loads data from twitter text file and create a dataset with new column names
        Dataset<Row> dataset = spark.read()
        		.option("inferSchema", true)
        		.csv("twitter2D.txt")
        		.withColumnRenamed("_c0", "Longitude")
        		.withColumnRenamed("_c1", "Latitude")
        		.withColumnRenamed("_c2", "Time")
        		.withColumnRenamed("_c3", "Val")
        		.withColumnRenamed("_c4", "Flag")
        		.withColumnRenamed("_c5", "Msg");
 
       // arranging all the null tweets to a new column Tweets and droping some of the columns
       Dataset<Row> cleanDataset = dataset.withColumn("Tweets", functions.coalesce(dataset.col("Msg"),dataset.col("Flag"))).drop("Msg")
    		   .drop("Flag");
       cleanDataset.show();
       //a transformer that combines a given list of columns into 
       //a single vector column. It is useful for combining raw features 
       //and features generated by different feature transformers into a 
       //single feature vector, in order to train ML models like logistic regression and decision trees.
        VectorAssembler assembler = new VectorAssembler()
        	      .setInputCols(new String[]{"Longitude","Latitude"})
        	      .setOutputCol("features");
        //obtaining a transformed vector into a dataset
        Dataset<Row> outputData = assembler.transform(cleanDataset);
        
        // Trains a k-means model and setting the number of clusters
        KMeans kmeans = new KMeans().setK(5).setSeed(1L);
        KMeansModel model = kmeans.fit(outputData);
        
        //creating a new column that holds the clusters based on the dataset provided 
        Dataset<Row> raw  = model.transform(outputData);
        raw.withColumnRenamed("prediction", "Cluster").show();
        
        // collect data from dataset columns ("prediction" and ("Msg")) as list and convert data to a string 
        String predColumn= raw.select("prediction").collectAsList().toString();
        String msgColumn = raw.select("Tweets").collectAsList().toString();
    
        // create string arrays from prediction and message tweets columns and split by commas
        TweetClusters.processingTweetsAndClusters(predColumn, msgColumn);       

        // Evaluate clustering by computing Within Set Sum of Squared Errors.
        double WSSSE = model.computeCost(outputData);
        System.out.println("Within Set Sum of Squared Errors = " + WSSSE);

        // Shows the result.
        Vector[] centers = model.clusterCenters();
        System.out.println("Cluster Centers: ");
        for (Vector center: centers) {
          System.out.println(center);
        }
                
    }

	
      
}
